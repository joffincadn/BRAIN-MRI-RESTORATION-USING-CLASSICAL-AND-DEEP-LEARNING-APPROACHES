# BRAIN-MRI-RESTORATION-USING-CLASSICAL-AND-DEEP-LEARNING-APPROACHES
This is a project done as Capstone Project for my final semester

**INTRODUCTION**

Magnetic Resonance Imaging (MRI) has revolutionized brain research and clinical diagnostics by providing high-resolution, non-invasive insights into the human brain's structure and function. It serves as a cornerstone in identifying and monitoring a wide range of neurological conditions, from tumors and strokes to degenerative diseases like Alzheimer's and multiple sclerosis. The quality and completeness of MRI scans are essential for making accurate diagnoses, guiding treatment decisions, and advancing scientific understanding of brain-related disorders.
Despite technological advancements, MRI data acquisition is not immune to imperfections. In many real-world scenarios, brain MRI scans can be partially corrupted or incomplete due to patient motion, equipment constraints, low signal quality, or data transfer issues. These missing or distorted regions, even when relatively small, can significantly impair visual interpretation and affect automated analysis workflows. As a result, there is a growing interest in techniques that can effectively address and compensate for missing or corrupted image data, particularly in clinical settings where re-scanning may not be possible.The challenge of restoring incomplete brain MRI scans lies not only in filling the gaps but also in doing so in a way that preserves anatomical accuracy and structural consistency. Unlike natural images, brain MRIs require reconstruction approaches that are sensitive to subtle intensity gradients, fine textures, and complex brain morphology. Furthermore, restoration quality must meet a high standard to be clinically meaningful—mere visual plausibility is not enough.
The restoration of missing information in brain MRI images represents a critical intersection of clinical relevance, data science, and image processing. It invites an exploration of both traditional and modern computational tools capable of understanding and reconstructing the complex patterns that define the human brain. The integration of these approaches into a reliable pipeline holds potential not only for improving image quality but also for enhancing trust in medical imaging systems and their downstream applications.

**DATA OVERVIEW**

The dataset utilized in this project is derived from the publicly available Br35H: Brain Tumor Detection 2020 collection hosted on Kaggle by Ahmed Hamada. For the scope of this study, only the non-tumorous MRI brain scans were selectively extracted to ensure a consistent and healthy brain anatomy representation across all samples. This curated subset forms the foundation for subsequent restoration and reconstruction experiments.
The dataset comprises approximately 1,500 brain MRI images, most of which are stored in RGB format, despite their grayscale visual appearance. A minority of images are encoded in other formats such as 'L' (grayscale), 'RGBA' , or 'P' modes. To maintain uniformity in preprocessing, these images are standardized by converting them to grayscale during the pipeline phase.
In terms of dimensional consistency, the dataset exhibits significant variability in image resolutions. The most frequently occurring dimension is 225×225 pixels, followed by a diverse mix of resolutions like `630×630`, `232×217`, and others. This heterogeneity necessitates image resizing as part of preprocessing, for which a standard size of 128×128 pixels is adopted to ensure computational efficiency and model compatibility. Despite the resolution differences, the aspect ratio distribution is largely centered around 1.0, suggesting most images are square or near-square in shape.
An analysis of pixel intensity characteristics reveals a broad range in brightness and contrast across the dataset. The mean pixel intensity varies between 20 and 120, with a majority of images clustered in the 50–80 range, reflecting moderate overall brightness. Similarly, the standard deviation of pixel intensities—indicative of contrast—exhibits a wide spread, with peaksaround 60–70, suggesting that many scans maintain moderate visual contrast, though some are notably flatter or more intense.

**Data Preparation and Preprocessing**

The dataset used in this project consists of non-tumorous brain MRI scans selected from the publicly available Br35H: Brain Tumor Detection 2020 dataset hosted on Kaggle by Ahmed Hamada. Only MRI images labeled as normal (i.e., without tumors) were extracted for this study to ensure consistency during reconstruction evaluation and avoid anatomical variability introduced by tumor presence.
The raw dataset folder was first scanned to list and load all image files with .jpg, .jpeg, or .png extensions. Each image was read using the Python Imaging Library (PIL), and immediately converted to grayscale (L mode), since MRI scans are naturally single-channel intensity-based images. This simplification reduces computational overhead and focuses the learning process on structural details rather than color variation.
A custom BrainMRIDataset class was developed using PyTorch’s Dataset module to:
Read and store image file paths
Apply transformation pipelines (like resizing and normalization)
Load and return each image as a tensor during iteration or training
To standardize input dimensions and optimize training performance, all images were resized to 128×128 pixels using bilinear interpolation. The resized images were then converted to PyTorch tensors using the ToTensor() transform, which also scales pixel intensities to a normalized range of [0, 1].
This preprocessing ensures that all images entering the model are of consistent size and intensity range, facilitating effective learning and reconstruction.
A visualization function was used to display random samples (Figure 1) from the processed dataset. This step served two purposes:
Verifying the integrity of the grayscale conversion and resizing
Visually inspecting the anatomical structure and quality of the MRI slices

<img width="1299" height="256" alt="image" src="https://github.com/user-attachments/assets/10af2871-1f00-4bc0-ab00-a19f0cad446d" />

**MODEL APPROACH**

To address the challenge of restoring incomplete brain MRI scans, this project adopts a three-pronged approach. First, a masking strategy is applied to simulate real-world data corruption by selectively removing regions from the images. Next, the RPCA (Robust Principal Component Analysis) model is employed as a classical technique to reconstruct the missing information by separating the low-rank structure from sparse noise. Finally, neural network-based models such as autoencoders and U-Net are utilized to learn complex mappings and enhance reconstruction accuracy through deep learning.

_Masking Technique_

To effectively evaluate and train reconstruction models, we must first simulate realistic data corruption in brain MRI images. This is achieved through a center-biased random masking technique, which artificially removes specific regions from the scans. The goal is to mimic scenarios where parts of the image are missing due to noise, motion artifacts, or sensor limitations, thereby creating a consistent input for reconstruction tasks.
Each MRI image from the dataset is first loaded using the Python Imaging Library (PIL) and converted into grayscale. This preprocessing step simplifies the data by reducing it to a single intensity channel, which is especially useful for medical imaging tasks where color is not a relevant factor. Grayscale images also reduce the memory footprint and training complexity for the models that follow.
To ensure masking occurs only in meaningful anatomical areas, a binary brain mask is created using intensity thresholding. The image is converted to a NumPy array and a pixel intensity threshold (typically set at 20) is applied using OpenCV’s cv2.threshold() method. This separates the brain tissue from the dark background, producing a binary mask where non-zero values represent the presence of the brain. This step ensures that masking avoids irrelevant areas like the background or image borders.
A key innovation in this masking strategy is the introduction of a center bias. Since abnormalities and diagnostically important features often appear in the central regions of brain MRIs, we prioritize these regions during masking. This is achieved by computing a Euclidean distance transform from the image center using np.indices(). A Gaussian weighting function is applied over the distance map to generate a smooth gradient, where the center of the image has the highest weighting. This probabilistic weighting is then multiplied with the binary brain mask to create a center-biased probability map for sampling.
To avoid errors caused by edge clipping during patch creation, we exclude boundary regions based on the patch size. A trimmed version of the binary brain mask is used so that each selected center pixel has enough space around it to accommodate a full square patch. The final mask used for sampling combines this trimmed brain mask with the center-biased probability weights, ensuring both anatomical relevance and technical correctness.
From the final probability map, a predefined number of patch centers (e.g., 6) are randomly sampled using np.random.choice() with their respective probabilities. Around each sampled center, a square patch of specified size (e.g., 5×5 pixels) is created. These patches represent regions that will be masked. A binary mask matrix is then constructed, where pixels within these patches are set to 1, indicating the areas to be corrupted. Finally, the mask is applied to the original image to simulate data loss. This is done by reducing the intensity of the selected pixels, effectively “removing” information from those areas. The operation masked_img = np.array(img) -(mask // 2) subtracts a scaled binary mask from the image, dimming or nullifying the selected regions without introducing unnatural artifacts. This results in a masked image (Figure 2) that serves as the input for reconstruction models.

<img width="1206" height="1181" alt="image" src="https://github.com/user-attachments/assets/9250f2ae-a5d2-4b8a-9a7c-18f4991d84ca" /> figure 2

_Approach 1 – RPCA Model using Inexact ALM_

In this project , first we use Robust Principal Component Analysis (RPCA), a technique designed for separating a data matrix into two components: a low-rank matrix and a sparse matrix. This is particularly valuable in medical image reconstruction tasks, such as brain MRI inpainting, where we aim to restore missing or corrupted regions using structural regularities.
We start by using a function called rpca_inpaint_from_tensor, which handles everything for a single MRI slice. This function first converts a PyTorch image tensor to a NumPy array and uses a helper method called brain_only_square_mask. This mask generator places random small square patches (like 3x3) only inside the actual brain region of the MRI using a simple thresholding method (cv2.threshold) to avoid masking irrelevant black background.
Once we have the binary mask (where 1s are missing pixels), the image is corrupted by setting those regions to zero. Now we apply the actual RPCA logic using the inexact_alm_rpca function. This implementation uses the Inexact Augmented Lagrangian Method (IALM) — a well-known algorithm for solving the optimization problem behind RPCA.
And here :
L (Low-rank matrix) is extracted using Singular Value Thresholding (SVD) — the core step that reconstructs the image by identifying the structured part of the matrix (i.e., brain tissue).
S (Sparse matrix) captures the noise or missing patches by applying a soft-thresholding technique.
We combine this information to produce the reconstructed image, where missing values are filled in from L, but observed values are kept as-is.
To assess the performance of the RPCA reconstruction, three widely-used image similarity metrics are calculated:
MAE (Mean Absolute Error): Measures the average absolute difference between original and reconstructed pixels in the masked area.
PSNR (Peak Signal-to-Noise Ratio): Reflects reconstruction quality, where higher values indicate better fidelity.
SSIM (Structural Similarity Index Measure): Captures perceptual similarity, considering luminance, contrast, and structure.
The SSIM scores are consistently above 0.98, which is excellent — this means the reconstructed images retain most of the important structural information, even after masking.
The MAE values range from 0.30 to 0.66 — not perfect, but very reasonable considering we zeroed out entire patches.
The PSNR values are on the lower side (between ~2.7 to 7.5 dB), which makes sense because masked areas introduce abrupt pixel-level noise — but again,
SSIM matters more for MRI since human vision and diagnostics rely on structure more than pixel accuracy.
The image comparison plots (Original vs Masked vs RPCA reconstruction) (Figure 3) visually demonstrate the effectiveness of RPCA in filling in brain regions with realistic textures and structures. Despite missing patches, the model successfully preserves anatomical integrity, confirming RPCA's robustness for medical image inpainting.

<img width="595" height="823" alt="image" src="https://github.com/user-attachments/assets/c17b060b-4b4d-4cd8-8caf-b1fa49fd1d30" />

_U- Net_

The U-Net architecture was implemented to reconstruct missing regions in brain MRI images by learning from masked input-output image pairs. This model is particularly suited for medical imaging tasks due to its unique encoder-decoder structure with skip connections, allowing it to efficiently learn both fine and coarse image features.
The U-Net was defined with a series of convolutional layers (Conv2d) and ReLU activations for downsampling the feature space, followed by ConvTranspose2d layers for upsampling. Crucially, skip connections were added between the encoder and decoder paths to preserve spatial information lost during downsampling. These were implemented by concatenating corresponding feature maps from the encoder into the decoder pipeline, enhancing detail recovery during reconstruction.
The model was trained using the Mean Squared Error (MSE) loss function, which measures pixel-level reconstruction accuracy. A learning rate of 0.001, batch size of 4, and 10 epochs were used during training. The optimization was performed using the Adam optimizer, and training was monitored using the epoch-wise loss trend plotted via matplotlib.
After training, the U-Net model was evaluated using three key image quality metrics:
•PSNR (Peak Signal-to-Noise Ratio): Indicates how close the reconstructed image is to the original in signal fidelity.
•SSIM (Structural Similarity Index): Assesses perceptual and structural similarity between original and reconstructed images.
MSE (Mean Squared Error): Measures average squared pixel difference.
The values demonstrate strong reconstruction quality, with high PSNR and SSIM indicating that the restored images are nearly identical to the originals both visually and structurally.
Using matplotlib.pyplot, the reconstructed outputs were plotted alongside the masked and original images(figure 4). These visuals confirm that U-Net successfully filled in missing regions with structurally accurate brain tissue. The boundaries, textures, and contrast in the reconstructed images were consistent with the original MRIs, validating both perceptual and quantitative performance of the model.

<img width="623" height="990" alt="image" src="https://github.com/user-attachments/assets/c3b0e59b-3313-4291-8b46-333c05345b14" />

_Autoencoder_

The autoencoder model implemented in this project was designed to learn how to reconstruct missing regions in brain MRI scans by leveraging the image’s internal structure and regularities. This was accomplished using a convolutional autoencoder, a neural network composed of two main parts: an encoder and a decoder. In the encoder, the input image (which has masked or missing regions) is progressively downsampled using nn.Conv2d layers, reducing its  spatial dimensions while capturing increasingly abstract features. Each convolutional layer is followed by a ReLU activation, which introduces non-linearity and allows the network to learn complex patterns in the image data.
The decoder section uses nn.ConvTranspose2d layers to upsample the compressed feature representation back to the original image resolution. This reconstruction process aims to fill in the missing parts of the image based on the patterns it has learned. Since this version of the autoencoder doesn’t use skip connections (unlike U-Net), it relies entirely on the compressed latent representation to guide reconstruction, which can lead to slightly blurred output, especially around edges and fine anatomical structures.
Training was performed using PyTorch, where the model was trained for 10 epochs with a batch size of 4 using the Adam optimizer and a learning rate of 0.001. The loss function used was nn.MSELoss, which calculates the Mean Squared Error between the reconstructed and original (unmasked) images. This choice of loss function ensures that the model penalizes pixel-wise differences, encouraging accurate intensity-level restoration. A training loop was written to process batches of images, perform forward and backward passes, and log the training loss. At the end of each epoch, the model was evaluated on a validation set.
To monitor performance, the training loss was plotted using matplotlib, showing a consistent downward trend across epochs. This curve confirms that the model was able to progressively reduce reconstruction error during training, indicating successful learning.
After training, the model’s performance was evaluated using three key image quality metrics:
•PSNR (Peak Signal-to-Noise Ratio) quantifies the fidelity of the reconstructed images in terms of signal quality.
•SSIM (Structural Similarity Index) measures how structurally similar the reconstructed image is to the original, considering brightness, contrast, and texture.
•MSE (Mean Squared Error) provides the average squared difference between the pixels of the original and reconstructed images.
On the validation set, the model achieved a PSNR of 24.79, an SSIM of 0.8423, and an MSE of 0.004168. These results indicate that the model produces reasonably accurate reconstructions with good perceptual similarity to the original images, although some fine details may be slightly smoothed due to the limitations of the architecture.
The model’s outputs were visualized alongside the original and masked images. Using matplotlib.pyplot, three-column plots were generated showing the input (masked) image, the autoencoder output, and the ground truth (original) image(figure 5). The reconstructions visually confirm that the autoencoder was able to fill in the missing regions of the brain scans with coherent anatomical structure. While the reconstructions were somewhat softer or blurrier compared to the original images, key brain regions such as the cortical boundaries and ventricles were reasonably well-restored, validating the model’s effectiveness.

<img width="1028" height="1422" alt="image" src="https://github.com/user-attachments/assets/8419abb9-0581-4953-8e80-8070ab2fb3e2" />

_CONCLUSION_

This study presents a thoughtful investigation into the challenge of restoring missing or corrupted regions in brain MRI scans, a task crucial for improving the quality and usability of medical imaging in both clinical and research contexts. Through a careful selection of non-tumorous MRI images, preprocessing steps, and innovative masking techniques, the project successfully simulates real-world degradation scenarios and evaluates the efficacy of three distinct image reconstruction methods: Robust Principal Component Analysis (RPCA), Autoencoders, and U-Net.
Each method was rigorously implemented and tested:
RPCA, as a classical unsupervised method, offers a fast and training-free solution. Despite its limitations in visual sharpness and pixel-level accuracy, it performed strongly in preserving structural information (SSIM ~0.99), demonstrating its relevance for quick, approximate restorations where time and resources are limited.
The U-Net model, with its encoder-decoder structure and skip connections, showed remarkable structural fidelity. It achieved high SSIM scores (~0.993) and produced visually convincing results with sharp boundaries, making it particularly suitable for tasks requiring anatomical precision. However, it lagged slightly in PSNR and MSE compared to the autoencoder, reflecting a trade-off between spatial detail and signal fidelity.
The Autoencoder emerged as the most balanced and effective approach. With the highest PSNR (24.79 dB) and the lowest MSE (0.0039), it offered accurate and clean reconstructions. Its simplicity, efficiency in training, and strong numerical and perceptual performance make it a compelling choice for brain MRI restoration. Even without skip connections, the autoencoder managed to recover fine anatomical structures with reasonable detail.The thorough comparative analysis validates the Autoencoder as the most effective method in this study, while also showcasing the strengths of U-Net and RPCA under different constraints. This framework lays a strong foundation for future work on improving and combining these methods for enhanced medical image recovery, potentially contributing to better diagnostic outcomes and more robust AI-assisted medical systems.
